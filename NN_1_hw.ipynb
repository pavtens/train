{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# без torch\n",
    "class Object:\n",
    "    def __init__(self, x1, x2, a0, a1, a2):\n",
    "        self.x1=x1\n",
    "        self.x2=x2\n",
    "        self.alpha0=a0\n",
    "        self.alpha1=a1\n",
    "        self.alpha2=a2\n",
    "    def forward(self): # синус как функция активации\n",
    "        total = self.alpha0 + self.alpha1*self.x1 + self.alpha2*self.x2\n",
    "        z = math.sin(total)\n",
    "        return z, total\n",
    "    def backword(self, *z_reals): #находим координаты градиента, MSE - функция потерь\n",
    "        grad1 = sum([2*(self.forward()[0]-i)*math.cos(self.forward()[1])*self.x1\n",
    "          for i in z_reals])  #dL/dz * dz/da1\n",
    "        grad2 = sum([2*(self.forward()[0]-i)*math.cos(self.forward()[1])*self.x2\n",
    "          for i in z_reals])  #dL/dz * dz/da2\n",
    "        return grad1, grad2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40.63919807170366, 81.27839614340732)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = Object(1,2,3,4,5)\n",
    "d.backword(17,17,17,19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.8862, grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward torch\n",
    "tensor = torch.tensor([1, 2, 3, 4, 5]).float()\n",
    "wieghts = nn.Parameter(torch.rand(5), requires_grad=True)\n",
    "\n",
    "tensor_output = wieghts @ tensor.t()\n",
    "activation = nn.ReLU()\n",
    "activation(tensor_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.9338], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward torch Linear\n",
    "tensor = torch.tensor([1, 2, 3, 4, 5]).float()\n",
    "\n",
    "linear = nn.Linear(5, 1, bias=True)\n",
    "linear(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9761], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward torch Linear & ReLU\n",
    "tensor = torch.tensor([1, 2, 3, 4, 5]).float()\n",
    "\n",
    "seq = nn.Sequential(\n",
    "    \n",
    "        nn.Linear(5, 1, bias=True),\n",
    "        nn.ReLU()    \n",
    ")\n",
    "\n",
    "seq(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0185], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward torch Linear & Siqmoid\n",
    "tensor = torch.tensor([1, 2, 3, 4, 5]).float()\n",
    "\n",
    "seq = nn.Sequential(\n",
    "    \n",
    "        nn.Linear(5, 1, bias=True),\n",
    "        nn.Sigmoid()    \n",
    ")\n",
    "\n",
    "seq(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object:\n",
    "    def __init__(self, *x):\n",
    "        self.x=torch.tensor(x).float()\n",
    "        \n",
    "    def forward(self): # синус как функция активации\n",
    "        xx = self.x\n",
    "        razmer = list(xx.size())[1]\n",
    "        seq = nn.Sequential(\n",
    "\n",
    "        nn.Linear(razmer, 1, bias=True),\n",
    "        nn.Sigmoid()    \n",
    "        )\n",
    "        return seq(self.x)\n",
    "    \n",
    "    def backward(self, target):\n",
    "        xx = self.x\n",
    "        razmer = list(xx.size())[1]\n",
    "        seq = nn.Sequential(\n",
    "\n",
    "        nn.Linear(razmer, 1, bias=True),\n",
    "        nn.Sigmoid()    \n",
    "        )\n",
    "\n",
    "        loss = torch.mean((target - xx) ** 2).float()\n",
    "        loss.backward()\n",
    "        return seq[0].weight.grad\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7732]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = Object([1,2,3])\n",
    "dd.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\brigh\\PycharmProjects\\train\\NN_1_hw.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brigh/PycharmProjects/train/NN_1_hw.ipynb#ch0000037?line=0'>1</a>\u001b[0m \u001b[39m# прошу подсказать, почему внутри функции не работает loss.backward\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brigh/PycharmProjects/train/NN_1_hw.ipynb#ch0000037?line=1'>2</a>\u001b[0m target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m,\u001b[39m10\u001b[39m,(\u001b[39m3\u001b[39m,))\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/brigh/PycharmProjects/train/NN_1_hw.ipynb#ch0000037?line=2'>3</a>\u001b[0m dd\u001b[39m.\u001b[39;49mbackward(target)\n",
      "\u001b[1;32mc:\\Users\\brigh\\PycharmProjects\\train\\NN_1_hw.ipynb Cell 9'\u001b[0m in \u001b[0;36mObject.backward\u001b[1;34m(self, target)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brigh/PycharmProjects/train/NN_1_hw.ipynb#ch0000032?line=17'>18</a>\u001b[0m seq \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brigh/PycharmProjects/train/NN_1_hw.ipynb#ch0000032?line=18'>19</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brigh/PycharmProjects/train/NN_1_hw.ipynb#ch0000032?line=19'>20</a>\u001b[0m nn\u001b[39m.\u001b[39mLinear(razmer, \u001b[39m1\u001b[39m, bias\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brigh/PycharmProjects/train/NN_1_hw.ipynb#ch0000032?line=20'>21</a>\u001b[0m nn\u001b[39m.\u001b[39mSigmoid()    \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brigh/PycharmProjects/train/NN_1_hw.ipynb#ch0000032?line=21'>22</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brigh/PycharmProjects/train/NN_1_hw.ipynb#ch0000032?line=23'>24</a>\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean((target \u001b[39m-\u001b[39m xx) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/brigh/PycharmProjects/train/NN_1_hw.ipynb#ch0000032?line=24'>25</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brigh/PycharmProjects/train/NN_1_hw.ipynb#ch0000032?line=25'>26</a>\u001b[0m \u001b[39mreturn\u001b[39;00m seq[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mgrad\n",
      "File \u001b[1;32mc:\\Users\\brigh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\brigh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# прошу подсказать, почему внутри функции не работает loss.backward\n",
    "target = torch.randint(0,10,(3,)).float()\n",
    "dd.backward(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7294, -1.4588, -2.1882, -2.9177, -3.6471]])\n"
     ]
    }
   ],
   "source": [
    "# backword входного слоя, верно понимаю?\n",
    "predict = seq(tensor)\n",
    "\n",
    "target = torch.randint(0,10,(5,)).float()\n",
    "loss = torch.mean((target - predict) ** 2)\n",
    "\n",
    "loss.backward()\n",
    "print(seq[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sigmoid' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\brigh\\PycharmProjects\\train\\NN_1_hw.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brigh/PycharmProjects/train/NN_1_hw.ipynb#ch0000033?line=4'>5</a>\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean((target \u001b[39m-\u001b[39m predict) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brigh/PycharmProjects/train/NN_1_hw.ipynb#ch0000033?line=6'>7</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/brigh/PycharmProjects/train/NN_1_hw.ipynb#ch0000033?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(seq[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39mgrad)\n",
      "File \u001b[1;32mc:\\Users\\brigh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1185\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1184\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1185\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1186\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sigmoid' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "# backword выходного слоя\n",
    "# вопрос: почему не считается? в теории ведь мы можем найти градиент от функции активации (сигмоиды)\n",
    "predict = seq(tensor)\n",
    "\n",
    "target = torch.randint(0,10,(5,)).float()\n",
    "loss = torch.mean((target - predict) ** 2)\n",
    "\n",
    "loss.backward()\n",
    "print(seq[1].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tanh' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\brigh\\PycharmProjects\\train\\NN_1_hw.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brigh/PycharmProjects/train/NN_1_hw.ipynb#ch0000029?line=13'>14</a>\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean((target \u001b[39m-\u001b[39m predict) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brigh/PycharmProjects/train/NN_1_hw.ipynb#ch0000029?line=15'>16</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/brigh/PycharmProjects/train/NN_1_hw.ipynb#ch0000029?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(seq[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39mgrad)\n",
      "File \u001b[1;32mc:\\Users\\brigh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1185\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1184\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1185\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1186\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tanh' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "# По тангенсу тоже не получается\n",
    "tensor = torch.tensor([1, 2, 3, 4, 5]).float()\n",
    "\n",
    "seq = nn.Sequential(\n",
    "    \n",
    "        nn.Linear(5, 1, bias=True),\n",
    "        nn.Tanh(),\n",
    "        nn.Sigmoid()    \n",
    ")\n",
    "\n",
    "predict = seq(tensor)\n",
    "\n",
    "target = torch.randint(0,10,(5,)).float()\n",
    "loss = torch.mean((target - predict) ** 2)\n",
    "\n",
    "loss.backward()\n",
    "print(seq[1].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 2 и 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDGMomentum:\n",
    "    def __init__(self, momentum, lr, model):\n",
    "        self.momentum = momentum\n",
    "        self.lr = lr\n",
    "        self.velocity = torch.zeros_like(model)\n",
    "        self.model = model.float()\n",
    "\n",
    "    def step(self, grad):\n",
    "        self.velocity = self.momentum * self.velocity - self.lr * grad\n",
    "        self.model += self.velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# найти пересечение с осью x линии y = 2 * x - 7 методом градиентного спуска\n",
    "# ответ: 3.5\n",
    "\n",
    "def func(x):\n",
    "    return (2 * x -7) ** 2\n",
    "           # (2*x - 7)**4\n",
    "\n",
    "def grad_f(x):\n",
    "    return 2 * (2 * x - 7) * 2\n",
    "           # 8 * (2*x - 7)**3\n",
    "\n",
    "def solver(init_x):\n",
    "    x = torch.tensor(init_x)\n",
    "    g = grad_f(x)\n",
    "    optim = SDGMomentum(0.9, 0.001, x)\n",
    "    for i in range(300):\n",
    "        optim.step(g)\n",
    "        g = grad_f(optim.model)\n",
    "    print(optim.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5000)\n"
     ]
    }
   ],
   "source": [
    "solver(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Адаптивный оптимизатор Adagrad\n",
    "\n",
    "class Adagrad:\n",
    "    def __init__(self, lr, model):\n",
    "        self.lr = lr\n",
    "        self.accumulated = torch.zeros_like(model).float()\n",
    "        self.model = model.float()\n",
    "\n",
    "    def step(self, grad):\n",
    "        self.accumulated += grad**2\n",
    "        self.model += -grad*(self.lr/self.accumulated**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оптимизатор Adagrad находит ответ с lr=30 и 100 итерациями гр.спуска из точки x=100\n",
    "\n",
    "def func(x):\n",
    "    return (2 * x -7) ** 2\n",
    "           # (2*x - 7)**4\n",
    "\n",
    "def grad_f(x):\n",
    "    return 2 * (2 * x - 7) * 2\n",
    "           # 8 * (2*x - 7)**3\n",
    "\n",
    "def solver(init_x):\n",
    "    x = torch.tensor(init_x)\n",
    "    g = grad_f(x)\n",
    "    optim = Adagrad(30.0, x)\n",
    "    for i in range(100):\n",
    "        optim.step(g)\n",
    "        g = grad_f(optim.model)\n",
    "    print(optim.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5000)\n"
     ]
    }
   ],
   "source": [
    "solver(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8750a921b84a1b624817dbfae33fa28d408c1c697f0b181b4279f7947f553d12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
